generate:
  sources:
    database: RAW # Database where to look for source tables
    # schemas: # List of schema names where to look for source tables
    #   - RAW
    sources_destination: "models/L1_inlets/{{schema}}/_{{schema}}.yml" # Where sources yml files will be generated
    models_destination: "models/L1_inlets/{{schema}}/stg_{{relation}}.sql" # Where models sql files will be generated
    model_props_destination: "models/L1_inlets/{{schema}}/stg_{{relation}}.yml" # Where models yml files will be generated
    update_strategy: update # Action to perform when a property file exists. Options: update, recreate, fail, ask
    templates_folder: ".dbt_coves/templates" # Folder where source generation jinja templates are located.
    flatten_json_fields: "no" # Action to perform when VARIANT / JSON field is encounted

  properties:
    destination: "{{model_folder_path}}/{{model_file_name}}.yml" # Where models yml files will be generated
    # You can specify a different path by declaring it explicitly, i.e.: "models/staging/{{model_file_name}}.yml"
    update_strategy: ask # Action to perform when a property file already exists. Options: update, recreate, fail, ask
    models: "models/" # Model(s) path where 'generate properties' will look for models for generation

  metadata:
    database: RAW
    # schemas: # List of schema names where to look for source tables
    #   - RAW
    # destination: "metadata.csv"

  airflow_dags:
    secrets_manager: datacoves

    yml_path: "/config/workspace/{{ env_var('DATACOVES__AIRFLOW_DAGS_YML_PATH') }}"
    dags_path: "/config/workspace/{{ env_var('DATACOVES__AIRFLOW_DAGS_PATH') }}"

    generators_params:
      AirbyteDbtGenerator:
        host: "{{ env_var('DATACOVES__AIRBYTE_HOST_NAME') }}"
        port: "{{ env_var('DATACOVES__AIRBYTE_PORT') }}"
        airbyte_conn_id: airbyte_connection

        dbt_project_path: "{{ env_var('DATACOVES__DBT_HOME') }}"
        run_dbt_compile: false
        run_dbt_deps: false

      AirbyteGenerator:
        host: "{{ env_var('DATACOVES__AIRBYTE_HOST_NAME') }}"
        port: "{{ env_var('DATACOVES__AIRBYTE_PORT') }}"
        airbyte_conn_id: airbyte_connection

      FivetranDbtGenerator:
        dbt_project_path: "{{ env_var('DATACOVES__DBT_HOME') }}"
        fivetran_conn_id: fivetran_connection

        api_key: "{{ secret('fivetran_api_key') }}"
        api_secret: "{{ secret('fivetran_api_secret') }}"

        run_dbt_compile: false
        run_dbt_deps: false

extract:
  airbyte:
    path: /config/workspace/load/airbyte
    host: "{{ env_var('DATACOVES__AIRBYTE_HOST_NAME') }}"
    port: "{{ env_var('DATACOVES__AIRBYTE_PORT') }}"

  fivetran:
    path: /config/workspace/extract/fivetran

load:
  airbyte:
    path: /config/workspace/load/airbyte
    host: "{{ env_var('DATACOVES__AIRBYTE_HOST_NAME') }}"
    port: "{{ env_var('DATACOVES__AIRBYTE_PORT') }}"
    secrets_manager: datacoves

  fivetran:
    path: /config/workspace/load/fivetran
    secrets_manager: datacoves
    run_connection_tests: true

blue_green:
  prod_db_env_var: DATACOVES__MAIN__DATABASE # This holds the name of the production database --- no default
  # staging_database: STAGING_DB # Optional name you want to give to the staging database --- no default
  # staging_suffix: STAGING # If not using staging_database above, {prod_db}_{staging_suffix} will be used --- default: STAGING
  drop_staging_db_at_start: false # Start blue-green by dropping staging db --- default false
  drop_staging_db_on_failure: false # Drop staging db if blue-green fails --- default false
  keep_staging_db_on_success: false # Keep staging db if blue-green succeeds --- default false
  # dbt_selector: "-s personal_loans" # dbt build arguments --- no default
  # full_refresh: true # Append --full-refresh to dbt command --- default false
  # defer: true # run in dbt in deferral mode by adding -s --state:modified+ --state logs --defer to the dbt command --- default false
